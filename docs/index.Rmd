---
title: "index"
author: "Laura Maria Cajiao Gonzalez, Arthur Morales, Richa Sirohi, Farzad Sunavala"
date: "11/01/2021"
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

Our team includes members with various backgrounds including, energy, technology, aeronautics, and military services. One thing we all have in common is a love for food. We decided to use our knowledge of data analytics to perform a PCA and Cluster analysis on one of our most common favorite foods, pizza. 

The dataset source comes from [dataworld](https://data.world/sdhilip/pizza-datasets).The business problem we are trying to solve is: 

>"Does the proportion of nutrients in pizza affect the brand of pizza?" 

We compiled a data dictionary to provide more insight about the dataset below:

The purpose of this exercise was three-fold:

1. **Descriptive Statistics** - Perform various data visualizations to further understand and explore our dataset.

2.  **PCA** - Perform PCA in order to reduce dimensionality by using each data point onto only the first few principal components to obtain lower-dimensional data while keeping as much of the data’s variation as possible.

3. **Clustering** - Perform data segmentation that partitions the data into several groups based on their similarity.

#### Conclusions: 

1. The dataset was overall a clean multivariate normally distributed with no outliers, null values, and little correlation between independent variables. We did have to convert 'brand' from numeric to factor since it was categorical data.

2. The PCA found that the amount of variability captured in two principal components was 92%.

3. Clustering with kmeans provided 3 clusters to be the best fit while agglomerative provided 4 clusters to be the best fit. The cophenetic distance between dendrogram distances and euclidean distances was 0.919.

## Descriptive Statistics
#### Load Packages
```{r echo=TRUE, eval=FALSE}
library(ggplot2)
library(factoextra)
library(FactoMineR)
library(corrplot)
library(e1071)
library(lattice)
library(caret)
library(cluster)
library(MASS)
library(purrr)
library(dplyr)
library(clValid)
```

#### Load Dataset
```{r cache=TRUE}
df <- read.csv("../Pizza.csv")
```

#### Sumarize Data
Let's display the first 10 rows of data to review and think about.
``` {r cache=TRUE}
# display first 10 rows of data
head(df, n=10)
```
How much data do we have? We have a general idea, but it is better to have a precise figure. If we have a lot of instances, we may need to work with a smaller sample of our data so that modeling is computationally tractable. If we have a vast number of attributes, we may need to select those that are most relevant. If we have more attributes than instances, we may need to select specific modeling techniques.
``` {r cache=TRUE}
# display the dimensions of the dataset
dim(df)
```
We also need t know the types of the attributes in our data. The types will indicate the types of further analysis, types of visualization and even the types of machine learning algorithms that we can use. However, for the scope of this project, we will not be doing any machine learning but solely exploratory analysis.
``` {r cache=TRUE}
# list types for each attribute
sapply(df, class)
```
We could also look at a summary for each attribute in our data. Note that factors are described as counts next to each class lable and numerical attributes are described using the min, 25th percentile, median, mean, 75th, percentile, and max properties. 
``` {r cache=TRUE}
# summarize the dataset
summary(df)
```
One thing that could help us tell if the data has a Gaussian (or nearly Gaussian) distribution if looking at the standard deviation. This could also help us in outlier detection if any values are more than 3 times the standard deviation from the mean are outside of 99.7% of the data. However, we will not use this method for outlier detection and will focus on that later.
``` {r cache=TRUE}
# calculate standard deviation for all numeric attributes
sapply(df[,3:9], sd)
```
If a distribution looks nearly-Gaussian but is pushed far left or right it is useful to know the skew. It's easier to tell skewness from plots such as histograms, but let's do a quick calculation. 
``` {r cache=TRUE}
# calculate standard deviation for all numeric attributes
skew <- apply(df[,3:9], 2, skewness)
print(skew)
```
It's also important to observe and think how attributes relate to each other. For numeric attributes, an excellent way to think about attribute-to attribute interactions is to calculate correlations for each pair of attributes.
``` {r cache=TRUE}
# calculate a correlation matrix for numeric variables
correlations <- cor(df[,3:9])
# dispaly the correlation matrix
print(correlations)
```
The above table shows all paris of attribute correlations for numerical data. For our case, let's use a threshold of above 0.8 and below -0.8 are watch points as they show a high correlation or high negative correlation respectively. 

#### Data Visualizations
Histograms are useful to get an indication of the distribution of an attribute. 
Density plots are also useful as they give a more abstract depiction of the distribution of each variable.
Box and whisker plots is a good way to look at outliers and the middle 50% of the data.
Bar plots are useful for showing a proportion of instances that belong to each category. In our case our categorical variable is the pizza brand. 
``` {r cache=TRUE}
# histograms
par(mfrow=c(3,3))
for(i in 3:9) {
  hist(df[,i], main=names(df)[i])
}
```
``` {r cache=TRUE}
# density plots
par(mfrow=c(3,3))
for(i in 3:9) {
  plot(density(df[,i]), main=names(df)[i])
}
```
``` {r cache=TRUE}
# box and whisker plots
par(mfrow=c(3,3))
for(i in 3:9) {
  boxplot(df[,i], main=names(df)[i])
}
```
``` {r cache=TRUE}
# bar plots for categorical 
par(mfrow=c(1,1))
for(i in 1:1) {
  counts <- table(df[,i])
  name <- names(df)[i]
  barplot(counts, main=name)
}
```

Multivariate plots are plots of the relationship or interaction between attributes. This will help in understanding more about the distribution, central tendency, and spread over groups of data.

We took a look at the correlations when summarizing our data, let's now create a correlation plot. 
``` {r cache=TRUE}
# multivariate, lets try correlation plot
corrplot(correlations, method="circle")
```

A scatter plot plots two variables together showing the relationship between the two.Let's create a scatter plot for all pairs of attributes in our dataset. We can also view a scatter plot matrix colored by class/brand.
``` {r cache=TRUE}
# need to convert classes to categorical factors 
df <- read.csv("../Pizza.csv", stringsAsFactors = TRUE)
# scatter plot matrix and wo/class
pairs(df)
# scatter plot matrix and w/class
pairs(brand~., data=df, col=df$brand)
```

Similarly, let's create a density plot by class to review the density distribution. 
``` {r cache=TRUE}
# density plot w/class
x <- df[,3:9]
y <- df[,1]
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
```

Lastly, let's review the boxplot distriubtions of each attribute by class value. This can give us a different perspective of how each attribute relates to the class value.
``` {r cache=TRUE}
# box and whisker plot w/class
x <- df[,3:9]
y <- df[,1]
featurePlot(x=x, y=y, plot="box")
```

Something we noticed was that there are multiple rows with the same brand name. This could be problematic in our PCA analysis. Let's quickly take a look at the unique values. 
``` {r cache=TRUE}
classes <- unique(df$brand, incomparables = FALSE)
uids <- unique(df$id, incomparables = FALSE)
print(classes)
length(uids)
# get the count of the unique values for pizza brand
table(df$brand)
```

Notice, that they are ids that are clearly duplicated which could cause a problem in our dataset. They are 291 unique ids but 300 observations in our dataset. 

## Principal Component Analysis
Principal Component Analysis (PCA) reduces the dimensions of a dataset while maintaining variability. To accomplish this, we replace existing variables with new ones that are a linear combination of the originals. The new y-axis is perpendicular to the new x-axis and the first PCA is a linear combo of the original and the second PCA is orthogonal. PCA thus takes advantage of correlation in the dataset to reduce the dataset.
                                                    
A loading is the correlation between a variable in the original dataset and a principal component. It tells how much information is common between them. A positive value means the two variables increase or decrease together while negative values indicate they performance in an opposite manner.

#### Load Dataset
``` {r cache=TRUE}
df <- read.csv("../Pizza.csv")
```


#### Drop Id and Brand
Let's drop ID and Brand since we won't need this for our PCA analysis.
``` {r cache=TRUE}
df.min <- df[c(-2)]
df.min <- df.min[c(-1)]
```

#### Correlations
A loading is the correlation between a variable in the original dataset and a principal component. It tells how much information is common between them. A positive value means the two variables increase or decrease together while negative values indicate they performance in an opposite manner.
``` {r cache=TRUE}
cor(df.min) #conduct PCA on correlation matrix to create variables on commensurate scale
```

#### PCA Algorithm
The following PCA plot shows the variables factors individual contributions to the plan construction. In essence, it illustrates how each variable from the samples will map on the PCA plane. Carb is the only variable with a negative impact on dimension 1 and ash, protein, and moisture have a negative impact on dimension 2, and all to the degree to which the arrow shows. Thus, the angle between the vectors is an approximation of the correlation between the variables. We can see where there is a relatively small angle, which means they are positively correlated.
``` {r cache=TRUE}
pizza.pca <-PCA(df.min, scale.unit=TRUE, graph=TRUE)
```

#### Eigenvalues
The eigenvalues help illustrate the amount of variation and are used to calculate the eigenvectors (i.e., the principal components) for the PCA mapping. Eigenvalues help determine the magnitude of the eigenvectors. The amount of variance each accounts for is included in the table below with this variance plotted on the Scree plot.
``` {r cache=TRUE}
pizza.pca$eig #print eigenvalues
```

#### Scree Plot
The Scree plot gives the variability represented by each of the principal components. While 3 is more ideal number of principal components, with 7 we still resulted in a favorable scree plot with an ideal elbow bent plot.
``` {r cache=TRUE}
fviz_eig(pizza.pca) #Scree plot
```

#### Correlation Circle
The correlation circle projects the variables into the principal component space. Correlation circles give a visual sense of how the variables and the principal components fit together. The length of the vector explains the amount of the variability and that variable is explained by the first two principal components. In this case, the first principal component is about the total scores across the pizza metrics. The second is about the pizza’s relative performance.
``` {r cache=TRUE}
fviz_pca_var(pizza.pca, col.var="cos2") #correlation circle with cos2
```

#### Biplot
Biplots graph the original data. Biplot gives a low dimensional representation of the data using the first two principal components as the transformed space. A vector is included for each original variable. Using the graph, we can see the correlation between variables by examining the measure of the angle between them and the correlation between each variable and each of the principal components represented. We can also see how the transformed data points relate to the principal components, the relative differences between the data points in the transformed space, and the amount of the variability in the individual points explained by the first two principal components. In the obtained Biplot, the variables are represented as vectors and the observations as the names of each Pizza attribute. The distance between the attributes represents the similarity between them. Attributes that are closer have a similar profile or category.
``` {r cache=TRUE}
fviz_pca_biplot(pizza.pca, col.var="cos2") #coordinates of individuals in PCA space and correlations between PCA and variables
```

#### Conclusion
In this case with the pizza data, there are some outliers but for the most part, pizzas felt into one of a few identifiable “clusters”. These groups on the plot indicate different brands the samples are from. High carb pizzas likely belong to brand G while high fat and sodium pizzas belong to brand A. Thus, the pizzas can be identified for the performance against each other in the many variables and in relation to other brands.

## Clustering
Let's take a look at kmeans, agglomerative, and divisive clustering on our dataset.

### Kmeans Clustering
#### Load Dataset
First, we load the data set. This is the original dataset obtained from data.world with no modifications done by the team. We will complete those which are necessary directly within R.
``` {r cache=TRUE}
df <- read.csv("../Pizza.csv", stringsAsFactors = TRUE)
```

#### Drop Id and Brand
As R has some difficulty dealing with rows with duplicate names (the brand identifiers in our case) it is necessary to clean the data and drop the “variables” we don’t need to analyze. These are the informational variables ‘brand’ and ‘id’.
``` {r cache=TRUE}
df <- df[c(-1, -2)]
```

#### Normalize Data
Moving forward, we need to normalize our data if we are to conduct a bona fide k-means clustering analysis.  Some of our data points are of a very large magnitude and without normalization to get them all on to the same scale the algorithm would fail to work properly.
``` {r cache=TRUE}
normalized.data <- scale(df)
```

#### Calculate Distances
We convert this new normalization to a data frame that can be used for analysis.Here we are calculating the Euclidean distances so we can get a good visual representation of the data as a matrix, and finally, a heat map.
``` {r cache=TRUE}
# change type to dataframe
newdf<-data.frame(normalized.data)
# Calculate distances
dist.pizza<-get_dist(newdf, method="euclidean")
# look at the heatmap of the data as a matrix
fviz_dist(dist.pizza)
```

#### Kmeans Algorithm
Now we will set the random seed for the purposes of this report to guarantee it is the same each time the code is run and begin the clustering analysis. Based on a comparison of the Within Cluster Sum of Squares (WCSS) and Between Cluster Sum of Squares (BCSS), we have decided to chose the amount of (k) clusters as k = 3. The best WCSS and BCSS came with 9 clusters, but this would make the analysis redundant and unnecessary as each of the 9 brands would essentially fall into their own cluster.  Choosing from 9 brands was the main issue in the first place.
``` {r cache=TRUE}
# set seed to guarantee reproducibility
set.seed(99) 
# centers is the #### number of clusters, nstart is the number of times you want the algorithm to run. 
PizzaCluster<-kmeans(newdf,centers=3,nstart=1)
```

#### Examine Results
Here we can see that the WCSS is 530 and the BCSS is 1563. These numbers are a significant improvement over an analysis with 2 clusters, and can give the consumer a relatively simple choice of pizza, as 9 brands are reduced into 3 groups.
``` {r cache=TRUE}
#examine results
str(PizzaCluster)
```

#### Plot Results
The k-means result plot gives a good visual representation of where the data falls when grouped into the 3 clusters based on the 7 content variables from the original dataset.
``` {r cache=TRUE}
#plot results
plot(newdf,col=PizzaCluster$cluster) 
# add the centers to your plot
points(PizzaCluster$centers, col=1:2,pch=8)
```

#### Evaluate Results
To see exactly which brands fall into which cluster, we reload the original dataset with the ‘brand’ variable and transpose the corresponding cluster number onto it. What results is a new data frame complete with brand identifier and cluster number. Here is a sample:
``` {r cache=TRUE}
##Add cluster identifier to dataframe to see where the clusters fall
#Reload Dataset to see brands again
df <- read.csv("../Pizza.csv", stringsAsFactors = TRUE)
kmeans_pizza_df <- data.frame(Cluster = PizzaCluster$cluster, df)
head(kmeans_pizza_df)
```

### Agglomerative Clustering 
For agglomerative clustering, we'll reuse many parts of the code we applied for kmeans but apply the agglomerative clustering algorithm instead.
``` {r cache=TRUE}
df <- read.csv("../Pizza.csv", stringsAsFactors = TRUE)
```

#### Drop Id and Brand
``` {r cache=TRUE}
df <- df[c(-1, -2)]
```

#### Normalize Data
``` {r cache=TRUE}
normalized.data <- scale(df)
```

#### Calculate Distances
``` {r cache=TRUE}
# change type to dataframe
newdf<-data.frame(normalized.data)
# Calculate distances
dist.pizza<-get_dist(newdf, method="euclidean")
# visualize to find any outliers
fviz_dist(dist.pizza)
```

#### Agglomerative Clustering Algoirthm
Let's carry out the agglomerative clustering here. We use different methods to assess the clustering. We used the agnes function to get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).
``` {r cache=TRUE}
# carry out hierarchical clustering
hc <- hclust(dist.pizza)
# Compute with agnes
hc2 <- agnes(newdf, method="complete")
# compute agglomerative coefficient
hc2$ac
```

##### Evaluate methods 
This allows us to find certain hierarchial clustering methods that can identify strong clustering structures. We find that Ward's method is the strongest.
``` {r cache=TRUE}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
# function to compute coefficient
ac <- function(x) {
  agnes(newdf, method = x)$ac
}
map_dbl(m, ac)
```

##### Dendrogram
A dendrogram is a branching diagram that represents the relationships of similarity among a group of entities. We can also view the merge and height values.
``` {r cache=TRUE}
fviz_dend(hc)

head(hc$merge, n=10)
head(hc$height, n=10)
```

Let's plot the dendrogram and create superimpose rectangular compartments for each cluster on the tree. We can also draw the cut line based on height.
``` {r cache=TRUE}
plot(hc)
# then draw superimpoes rectangular compartments for each cluster on the tree
rect.hclust(hc, k=4, border = 2:6)

# can draw th cut line 
abline(h=3, col='yellow')
```

Let's now find the length of the unique clusters. 
``` {r cache=TRUE}
# cut the tree by number of cluster: k
result_cluster_k <- cutree(hc, k=4)
# cut the tree by height: h 
result_cluster <- cutree(hc, h=4)
length(unique(result_cluster))
```

Let's take a look at how many items are in each cluster.
``` {r cache=TRUE}
#add additional column
df_cl <- mutate(newdf, cluster =result_cluster)
# report how many items in each cluster
count(df_cl, cluster)
```

#### Examine Results
Now we can examine the results and find the optimum number of clusters and how it relates to our results.Let's use the cophenetic distance, elbow method (WSS), and dunn index.
``` {r cache=TRUE}
# optimum number of clusters
fviz_nbclust(newdf, FUN = hcut, method = "wss")
# dunn index
result_cluster <- cutree(hc, k=4)
dunn(get_dist(normalized.data, method = "euclidean"), result_cluster)
#plot clusters
plot(newdf, col=result_cluster_k)
# distances from the dendrogram
pizza.coph <- cophenetic(hc)
# visualize dengrogram distances with a heatmap
fviz_dist(pizza.coph)
# correlation between dendrogram distances and euclidean distances
cor(get_dist(newdf, method="euclidean"),pizza.coph)
```

#### Conclusion
Clustering showed to be a very useful tool in the unsupervised learning. That being said, we realized a number of isses that arose in clustering.In summary, we found that in hiearchical agglomerative clustering was best when the variables are separated into 4 clusters. 
